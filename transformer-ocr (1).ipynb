{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10996082,"sourceType":"datasetVersion","datasetId":6844200}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:13:32.438117Z","iopub.execute_input":"2025-03-13T06:13:32.438322Z","iopub.status.idle":"2025-03-13T06:13:32.442283Z","shell.execute_reply.started":"2025-03-13T06:13:32.438301Z","shell.execute_reply":"2025-03-13T06:13:32.441476Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install torch torchvision numpy pillow h5py opencv-python xmltodict\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:21:54.845574Z","iopub.execute_input":"2025-03-13T06:21:54.845891Z","iopub.status.idle":"2025-03-13T06:21:58.356241Z","shell.execute_reply.started":"2025-03-13T06:21:54.845868Z","shell.execute_reply":"2025-03-13T06:21:58.355412Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.12.1)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nCollecting xmltodict\n  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\nInstalling collected packages: xmltodict\nSuccessfully installed xmltodict-0.14.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport xml.etree.ElementTree as ET\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\nfrom pathlib import Path\nimport numpy as np\nimport math\nfrom itertools import groupby\nimport h5py\nimport numpy as np\nimport unicodedata\nimport cv2\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet50, resnet101\nfrom torch.autograd import Variable\nimport torchvision\n\nfrom torch.utils.data import Dataset\nimport time\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:11.601064Z","iopub.execute_input":"2025-03-13T07:00:11.601353Z","iopub.status.idle":"2025-03-13T07:00:11.606558Z","shell.execute_reply.started":"2025-03-13T07:00:11.601332Z","shell.execute_reply":"2025-03-13T07:00:11.605465Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=128):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass OCR(nn.Module):\n\n    def __init__(self, vocab_len, hidden_dim, nheads,\n                 num_encoder_layers, num_decoder_layers):\n        super().__init__()\n\n        # create ResNet-101 backbone\n        self.backbone = resnet101()\n        del self.backbone.fc\n\n        # create conversion layer\n        self.conv = nn.Conv2d(2048, hidden_dim, 1)\n\n        # create a default PyTorch transformer\n        self.transformer = nn.Transformer(\n            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n\n        # prediction heads with length of vocab\n        # DETR used basic 3 layer MLP for output\n        self.vocab = nn.Linear(hidden_dim,vocab_len)\n\n        # output positional encodings (object queries)\n        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n        self.query_pos = PositionalEncoding(hidden_dim, .2)\n\n        # spatial positional encodings, sine positional encoding can be used.\n        # Detr baseline uses sine positional encoding.\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.trg_mask = None\n  \n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask==1, float('-inf'))\n        return mask\n\n    def get_feature(self,x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)   \n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n        return x\n\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n\n    def forward(self, inputs, trg):\n        # propagate inputs through ResNet-101 up to avg-pool layer\n        x = self.get_feature(inputs)\n\n        # convert from 2048 to 256 feature planes for the transformer\n        h = self.conv(x)\n\n        # construct positional encodings\n        bs,_,H, W = h.shape\n        pos = torch.cat([\n            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n        ], dim=-1).flatten(0, 1).unsqueeze(1)\n\n        # generating subsequent mask for target\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n\n        # Padding mask\n        trg_pad_mask = self.make_len_mask(trg)\n\n        # Getting postional encoding for target\n        trg = self.decoder(trg)\n        trg = self.query_pos(trg)\n        \n        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1,0,2), tgt_mask=self.trg_mask, \n                                  tgt_key_padding_mask=trg_pad_mask.permute(1,0))\n\n        return self.vocab(output.transpose(0,1))\n\n\ndef make_model(vocab_len, hidden_dim=256, nheads=4,\n                 num_encoder_layers=4, num_decoder_layers=4):\n    \n    return OCR(vocab_len, hidden_dim, nheads,\n                 num_encoder_layers, num_decoder_layers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:12.663198Z","iopub.execute_input":"2025-03-13T07:00:12.663532Z","iopub.status.idle":"2025-03-13T07:00:12.676089Z","shell.execute_reply.started":"2025-03-13T07:00:12.663494Z","shell.execute_reply":"2025-03-13T07:00:12.675303Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass PromitoLipiDataset(Dataset):\n    def __init__(self, img_dir, annotation_file, tokenizer, transform=None):\n        self.img_dir = img_dir\n        self.annotations = load_annotations(annotation_file)  # Load annotations\n        self.image_files = list(self.annotations.keys())  # Get image filenames\n        self.tokenizer = tokenizer  # Use the Tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]\n        label_text = self.annotations[image_name]  # Get Bangla text label\n\n        # Load Image\n        img_path = os.path.join(self.img_dir, image_name)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            img = self.transform(img)\n\n        # Convert text to token indices\n        y_train = self.tokenizer.encode(label_text)\n\n        # Pad sequence to max length\n        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)), mode=\"constant\")\n\n        return img, torch.tensor(y_train, dtype=torch.long)\n\n\n\n# Image Transformations for ResNet-101\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Resize images\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:12.865796Z","iopub.execute_input":"2025-03-13T07:00:12.866065Z","iopub.status.idle":"2025-03-13T07:00:12.872758Z","shell.execute_reply.started":"2025-03-13T07:00:12.866044Z","shell.execute_reply":"2025-03-13T07:00:12.871867Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"class Tokenizer:\n    \"\"\"Handles text-token conversion for Bangla OCR.\"\"\"\n\n    def __init__(self, class_mapping, max_text_length=128):\n        self.PAD_TK, self.UNK_TK, self.SOS_TK, self.EOS_TK = \"¶\", \"¤\", \"SOS\", \"EOS\"\n        \n        # Convert class_mapping to a list of characters\n        self.chars = [self.PAD_TK, self.UNK_TK, self.SOS_TK, self.EOS_TK] + list(class_mapping.values())\n        self.vocab_size = len(self.chars)\n        self.maxlen = max_text_length\n\n        # Create character-to-index and index-to-character mappings\n        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}\n        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n\n    def encode(self, text):\n        \"\"\"Encodes Bangla text into token indices.\"\"\"\n        text = ['SOS'] + list(text) + ['EOS']\n        encoded = [self.char_to_idx.get(c, self.char_to_idx[\"¤\"]) for c in text]\n        return np.array(encoded)\n\n    def decode(self, tokens):\n        \"\"\"Decodes token indices back to Bangla text.\"\"\"\n        text = \"\".join([self.idx_to_char.get(i, \"\") for i in tokens])\n        return text.replace(\"SOS\", \"\").replace(\"EOS\", \"\").replace(\"¶\", \"\").replace(\"¤\", \"\")\n\n\n\n# # Bangla Charset (Letters, Digits, and Common Symbols)\n# bangla_charset = \"অইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহড়ঢ়য়ৎঃংঁ\" \\\n#                  \"০১২৩৪৫৬৭৮৯\" \\\n#                  \"ািীেু্র্যক্ষন্তত্রঙ্গস্থস্বক্তস্তন্দচ্ছদ্ধন্ত্রত্তষ্টন্নল্পম্পূৃৈৌ।\"\n\nclass_mapping = {\n    0: 'blank', 1: 'অ', 2: 'ই', 3: 'ঈ', 4: 'উ', 5: 'ঊ', 6: 'ঋ', 7: 'এ', 8: 'ঐ', 9: 'ও', 10: 'ঔ',\n    11: 'ক', 12: 'খ', 13: 'গ', 14: 'ঘ', 15: 'ঙ', 16: 'চ', 17: 'ছ', 18: 'জ', 19: 'ঝ', 20: 'ঞ', 21: 'ট',\n    22: 'ঠ', 23: 'ড', 24: 'ঢ', 25: 'ণ', 26: 'ত', 27: 'থ', 28: 'দ', 29: 'ধ', 30: 'ন', 31: 'প', 32: 'ফ',\n    33: 'ব', 34: 'ভ', 35: 'ম', 36: 'য', 37: 'র', 38: 'ল', 39: 'শ', 40: 'ষ', 41: 'স', 42: 'হ', 43: 'ড়',\n    44: 'ঢ়', 45: 'য়', 46: 'ৎ', 47: 'ঃ', 48: 'ং', 49: 'ঁ', 50: '০', 51: '১', 52: '২', 53: '৩', 54: '৪',\n    55: '৫', 56: '৬', 57: '৭', 58: '৮', 59: '৯', 60: 'া', 61: 'ি', 62: 'ী', 63: 'ে', 64: 'ু', 65: 'faka',\n    66: '্র', 67: '্য', 68: 'ক্ষ', 69: 'ন্ত', 70: 'ত্র', 71: 'ঙ্গ', 72: 'স্থ', 73: 'স্ব', 74: 'ক্ত',\n    75: 'স্ত', 76: 'ন্দ', 77: 'চ্ছ', 78: 'দ্ধ', 79: 'ন্ত্র', 80: 'ফাকা', 81: 'ত্ত', 82: 'ষ্ট', 83: 'ন্ন',\n    84: 'ল্প', 85: 'ম্প', 86: 'faka', 87: 'ূ', 88: 'ৃ', 89: 'ৈ', 90: 'faka', 91: 'ৌ', 92: '।'\n}\n\n# Initialize Tokenizer with Bangla characters\ntokenizer = Tokenizer(class_mapping)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:13.040791Z","iopub.execute_input":"2025-03-13T07:00:13.041058Z","iopub.status.idle":"2025-03-13T07:00:13.051232Z","shell.execute_reply.started":"2025-03-13T07:00:13.041037Z","shell.execute_reply":"2025-03-13T07:00:13.050422Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"# Encoding Example\ntext = \"বাংলা ভাষা\"\nencoded_text = tokenizer.encode(text)\nprint(\"Encoded:\", encoded_text)\n\n# Decoding Example\ndecoded_text = tokenizer.decode(encoded_text)\nprint(\"Decoded:\", decoded_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:14.877290Z","iopub.execute_input":"2025-03-13T07:00:14.877654Z","iopub.status.idle":"2025-03-13T07:00:14.883083Z","shell.execute_reply.started":"2025-03-13T07:00:14.877625Z","shell.execute_reply":"2025-03-13T07:00:14.882359Z"}},"outputs":[{"name":"stdout","text":"Encoded: [ 2 37 64 52 42 64  1 38 64 44 64  3]\nDecoded: বাংলাভাষা\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"import os\n\ndef load_annotations(annotation_file):\n    \"\"\"Loads annotations from a text file and converts class indices to Bangla text.\"\"\"\n    annotations = {}\n\n    # Check if the file exists\n    if not os.path.exists(annotation_file):\n        print(f\"❌ Error: Annotation file not found at {annotation_file}\")\n        return annotations  # Return empty dictionary\n\n    with open(annotation_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            parts = line.strip().split(\":\")\n            if len(parts) != 2:\n                print(f\"⚠️ Warning: Skipping malformed line: {line.strip()}\")\n                continue  # Skip corrupt lines\n\n            image_name = parts[0].strip()  # Extract image filename\n\n            try:\n                # Convert index numbers to actual Bangla text\n                label_indices = list(map(int, parts[1].strip().split()))\n                label_text = \"\".join(class_mapping.get(idx, \"\") for idx in label_indices)\n                annotations[image_name] = label_text  # Store result\n            except ValueError:\n                print(f\"⚠️ Warning: Skipping line with invalid numbers: {line.strip()}\")\n\n    return annotations\n\n# Load annotations from file\nannotation_file_path = \"/kaggle/input/promitilipi/imageannotationsid_train.txt\"\nannotations = load_annotations(annotation_file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:15.915179Z","iopub.execute_input":"2025-03-13T07:00:15.915493Z","iopub.status.idle":"2025-03-13T07:00:15.940997Z","shell.execute_reply.started":"2025-03-13T07:00:15.915468Z","shell.execute_reply":"2025-03-13T07:00:15.940345Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"batch_size = 16\n\n# Example Usage:\ndataset = PromitoLipiDataset(\n    img_dir=\"/kaggle/input/promitilipi/promitilipi/preprocessed_images\",\n    annotation_file=annotation_file_path,\n    tokenizer=tokenizer,  # Pass the tokenizer\n    transform=transforms.Compose([transforms.ToTensor()])\n)\n\n# Check some samples\nfor i in range(5):\n    img, label = dataset[i]\n    print(f\"Sample {i+1}: {tokenizer.decode(label.numpy())}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:18.234998Z","iopub.execute_input":"2025-03-13T07:00:18.235288Z","iopub.status.idle":"2025-03-13T07:00:18.271089Z","shell.execute_reply.started":"2025-03-13T07:00:18.235265Z","shell.execute_reply":"2025-03-13T07:00:18.270407Z"}},"outputs":[{"name":"stdout","text":"Sample 1: উপকরণ\nSample 2: উপবন\nSample 3: ঊহয\nSample 4: উদাস\nSample 5: উপজািত\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# Define dataset\n# dataset = PromitoLipiDataset(\n#     img_dir=\"/kaggle/input/promitilipi/PromitoLipi2/PromitoLipi2/WordImages(bmp)\",\n#     xml_dir=\"/kaggle/input/promitilipi/PromitoLipi2/PromitoLipi2/WordAnnotations(xml)\",\n#     tokenizer=tokenizer,\n#     transform=transform\n# )\n\n# Define split sizes (80% Train, 10% Validation, 10% Test)\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size  # Ensures total remains same\n\n# Randomly split dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# Create DataLoaders\nbatch_size = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Print dataset sizes\nprint(f\"Training Set: {len(train_dataset)} samples\")\nprint(f\"Validation Set: {len(val_dataset)} samples\")\nprint(f\"Test Set: {len(test_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:20.570359Z","iopub.execute_input":"2025-03-13T07:00:20.570726Z","iopub.status.idle":"2025-03-13T07:00:20.578862Z","shell.execute_reply.started":"2025-03-13T07:00:20.570698Z","shell.execute_reply":"2025-03-13T07:00:20.578120Z"}},"outputs":[{"name":"stdout","text":"Training Set: 7864 samples\nValidation Set: 983 samples\nTest Set: 983 samples\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"# Define the class mapping\n\n\n\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:20.758112Z","iopub.execute_input":"2025-03-13T07:00:20.758415Z","iopub.status.idle":"2025-03-13T07:00:20.761666Z","shell.execute_reply.started":"2025-03-13T07:00:20.758391Z","shell.execute_reply":"2025-03-13T07:00:20.760858Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:22.055980Z","iopub.execute_input":"2025-03-13T07:00:22.056267Z","iopub.status.idle":"2025-03-13T07:00:22.061121Z","shell.execute_reply.started":"2025-03-13T07:00:22.056245Z","shell.execute_reply":"2025-03-13T07:00:22.060285Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"97"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = make_model(vocab_len=tokenizer.vocab_size).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:23.126875Z","iopub.execute_input":"2025-03-13T07:00:23.127161Z","iopub.status.idle":"2025-03-13T07:00:23.934422Z","shell.execute_reply.started":"2025-03-13T07:00:23.127139Z","shell.execute_reply":"2025-03-13T07:00:23.933479Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"class LabelSmoothing(nn.Module):\n    \"Implement label smoothing.\"\n    def __init__(self, size, padding_idx=0, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False)\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        \n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, Variable(true_dist, requires_grad=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:23.935734Z","iopub.execute_input":"2025-03-13T07:00:23.935996Z","iopub.status.idle":"2025-03-13T07:00:23.941878Z","shell.execute_reply.started":"2025-03-13T07:00:23.935974Z","shell.execute_reply":"2025-03-13T07:00:23.941001Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"criterion = LabelSmoothing(size=tokenizer.vocab_size, padding_idx=0, smoothing=0.1).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.0004)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:00:24.413625Z","iopub.execute_input":"2025-03-13T07:00:24.413894Z","iopub.status.idle":"2025-03-13T07:00:24.421096Z","shell.execute_reply.started":"2025-03-13T07:00:24.413873Z","shell.execute_reply":"2025-03-13T07:00:24.420250Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"def train(model, criterion, optimiser, scheduler,dataloader):\n \n    model.train()\n    total_loss = 0\n    for batch, (imgs, labels_y,) in enumerate(dataloader):\n          imgs = imgs.to(device)\n          labels_y = labels_y.to(device)\n    \n          optimiser.zero_grad()\n          output = model(imgs.float(),labels_y.long()[:,:-1])\n \n          norm = (labels_y != 0).sum()\n          loss = criterion(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size), labels_y[:,1:].contiguous().view(-1).long()) / norm\n \n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n          optimizer.step()\n          total_loss += loss.item() * norm\n \n    return total_loss / len(dataloader)\n \nfrom jiwer import cer\n\ndef evaluate(model, criterion, dataloader, tokenizer):\n    model.eval()\n    epoch_loss = 0\n    total_cer = 0  # Track total CER\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y,) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n\n            # Model prediction\n            output = model(imgs.float(), labels_y.long()[:, :-1])\n\n            # Compute loss\n            norm = (labels_y != 0).sum()\n            loss = criterion(\n                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n                labels_y[:, 1:].contiguous().view(-1).long()\n            ) / norm\n\n            epoch_loss += loss.item() * norm\n\n            # Convert model output to text (Decoding)\n            predicted_tokens = output.argmax(-1).cpu().numpy()  # Get best predictions\n            predicted_texts = [tokenizer.decode(pred) for pred in predicted_tokens]  # Convert to text\n            actual_texts = [tokenizer.decode(label.cpu().numpy()) for label in labels_y[:, 1:]]  # Ground truth text\n\n            # Compute CER for this batch\n            batch_cer = sum(cer(a, b) for a, b in zip(actual_texts, predicted_texts)) / len(actual_texts)\n            total_cer += batch_cer\n            total_samples += 1\n\n    # Average CER across all validation samples\n    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n    return epoch_loss / len(dataloader), avg_cer\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:07:42.485283Z","iopub.execute_input":"2025-03-13T07:07:42.485666Z","iopub.status.idle":"2025-03-13T07:07:42.518016Z","shell.execute_reply.started":"2025-03-13T07:07:42.485636Z","shell.execute_reply":"2025-03-13T07:07:42.517108Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"!pip install jiwer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:06:52.772724Z","iopub.execute_input":"2025-03-13T07:06:52.773054Z","iopub.status.idle":"2025-03-13T07:06:58.292860Z","shell.execute_reply.started":"2025-03-13T07:06:52.773025Z","shell.execute_reply":"2025-03-13T07:06:58.291769Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nCollecting click>=8.1.8 (from jiwer)\n  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\nDownloading click-8.1.8-py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rapidfuzz-3.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, click, jiwer\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\nSuccessfully installed click-8.1.8 jiwer-3.1.0 rapidfuzz-3.12.2\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"#train model\n \ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\ntarget_path = \"/kaggle/working/best_model.pth\"  # ✅ Define the path\n\n\nbest_valid_loss = np.inf\nc = 0\nfor epoch in range(200):\n    print(f'Epoch: {epoch+1:02}, Learning Rate: {scheduler.get_last_lr()}')\n\n    start_time = time.time()\n\n    train_loss = train(model, criterion, optimizer, scheduler, train_loader)\n    valid_loss, valid_cer = evaluate(model, criterion, val_loader, tokenizer)  # ✅ Now returns CER\n\n    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n\n    c += 1\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), target_path)\n        c = 0\n\n    if c > 4:\n        scheduler.step()\n        c = 0\n\n    print(f'Time: {epoch_mins}m {epoch_secs}s')\n    print(f'Train Loss: {train_loss:.3f}')\n    print(f'Val   Loss: {valid_loss:.3f}')\n    print(f'CER   Score: {valid_cer:.3f}')  # ✅ Display Character Error Rate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T07:09:07.849225Z","iopub.execute_input":"2025-03-13T07:09:07.849558Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01, Learning Rate: [0.0001]\nTime: 0m 58s\nTrain Loss: 104.137\nVal   Loss: 97.942\nCER   Score: 0.689\nEpoch: 02, Learning Rate: [0.0001]\nTime: 1m 12s\nTrain Loss: 100.143\nVal   Loss: 92.267\nCER   Score: 1.414\nEpoch: 03, Learning Rate: [0.0001]\nTime: 0m 57s\nTrain Loss: 95.871\nVal   Loss: 87.957\nCER   Score: 0.735\nEpoch: 04, Learning Rate: [0.0001]\nTime: 0m 58s\nTrain Loss: 91.931\nVal   Loss: 86.311\nCER   Score: 0.563\nEpoch: 05, Learning Rate: [0.0001]\nTime: 0m 57s\nTrain Loss: 88.519\nVal   Loss: 84.072\nCER   Score: 0.735\nEpoch: 06, Learning Rate: [0.0001]\nTime: 1m 12s\nTrain Loss: 85.431\nVal   Loss: 81.932\nCER   Score: 0.735\nEpoch: 07, Learning Rate: [0.0001]\nTime: 1m 0s\nTrain Loss: 82.582\nVal   Loss: 79.282\nCER   Score: 0.540\nEpoch: 08, Learning Rate: [0.0001]\nTime: 0m 57s\nTrain Loss: 79.191\nVal   Loss: 78.191\nCER   Score: 0.490\nEpoch: 09, Learning Rate: [0.0001]\nTime: 0m 58s\nTrain Loss: 77.045\nVal   Loss: 77.129\nCER   Score: 0.464\nEpoch: 10, Learning Rate: [0.0001]\nTime: 1m 3s\nTrain Loss: 74.380\nVal   Loss: 75.053\nCER   Score: 0.473\nEpoch: 11, Learning Rate: [0.0001]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}