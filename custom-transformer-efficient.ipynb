{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11021979,"sourceType":"datasetVersion","datasetId":6844200}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:17:19.550582Z","iopub.execute_input":"2025-03-24T17:17:19.550795Z","iopub.status.idle":"2025-03-24T17:17:19.554667Z","shell.execute_reply.started":"2025-03-24T17:17:19.550773Z","shell.execute_reply":"2025-03-24T17:17:19.553790Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision numpy pillow h5py opencv-python xmltodict efficientnet-pytorch\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:17:31.884831Z","iopub.execute_input":"2025-03-24T17:17:31.885216Z","iopub.status.idle":"2025-03-24T17:17:38.149075Z","shell.execute_reply.started":"2025-03-24T17:17:31.885184Z","shell.execute_reply":"2025-03-24T17:17:38.148016Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.12.1)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nCollecting xmltodict\n  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\nCollecting efficientnet-pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\nBuilding wheels for collected packages: efficientnet-pytorch\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=cae65110fd3fa2071a7f731c6984613e76a774bd4375b550b1b597f7c38eb7de\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet-pytorch\nInstalling collected packages: xmltodict, efficientnet-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.1 xmltodict-0.14.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport torch\nimport xml.etree.ElementTree as ET\nimport numpy as np\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nimport torch.nn.functional as F\n\nfrom pathlib import Path\nimport numpy as np\nimport math\nfrom itertools import groupby\nimport h5py\nimport numpy as np\nimport unicodedata\nimport cv2\nimport torch\nfrom torch import nn\nfrom torchvision.models import resnet50, resnet101\nfrom torch.autograd import Variable\nimport torchvision\nfrom efficientnet_pytorch import EfficientNet\n\nfrom torch.utils.data import Dataset\nimport time\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:17:42.318834Z","iopub.execute_input":"2025-03-24T17:17:42.319175Z","iopub.status.idle":"2025-03-24T17:17:46.616309Z","shell.execute_reply.started":"2025-03-24T17:17:42.319146Z","shell.execute_reply":"2025-03-24T17:17:46.615417Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\nfrom efficientnet_pytorch import EfficientNet  # EfficientNet from the efficientnet-pytorch library\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=128):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\n\nclass OCR(nn.Module):\n\n    def __init__(self, vocab_len, hidden_dim, nheads,\n                 num_encoder_layers, num_decoder_layers):\n        super().__init__()\n\n        # Use EfficientNet as the backbone\n        self.backbone = EfficientNet.from_pretrained('efficientnet-b7')  # You can choose other variants like b0, b1, etc.\n        # Remove the classification head (fully connected layer) from EfficientNet\n        self.backbone._fc = nn.Identity()  # The last fully connected layer is removed.\n\n        # Create conversion layer\n        self.conv = nn.Conv2d(2560, hidden_dim, 1)  # Change 2560 to the output channel size of EfficientNet\n\n        # Create a default PyTorch transformer\n        self.transformer = nn.Transformer(\n            hidden_dim, nheads, num_encoder_layers, num_decoder_layers)\n\n        # Prediction heads with length of vocab\n        self.vocab = nn.Linear(hidden_dim, vocab_len)\n\n        # Output positional encodings (object queries)\n        self.decoder = nn.Embedding(vocab_len, hidden_dim)\n        self.query_pos = PositionalEncoding(hidden_dim, .2)\n\n        # Spatial positional encodings, sine positional encoding can be used.\n        # Detr baseline uses sine positional encoding.\n        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.col_embed = nn.Parameter(torch.rand(50, hidden_dim // 2))\n        self.trg_mask = None\n  \n    def generate_square_subsequent_mask(self, sz):\n        mask = torch.triu(torch.ones(sz, sz), 1)\n        mask = mask.masked_fill(mask==1, float('-inf'))\n        return mask\n\n    def get_feature(self,x):\n        x = self.backbone.extract_features(x)  # Use the EfficientNet feature extraction method\n        return x\n\n\n    def make_len_mask(self, inp):\n        return (inp == 0).transpose(0, 1)\n\n\n    def forward(self, inputs, trg):\n        # Propagate inputs through EfficientNet backbone\n        x = self.get_feature(inputs)\n\n        # Convert from 2560 (EfficientNet output size) to 256 feature planes for the transformer\n        h = self.conv(x)\n\n        # Construct positional encodings\n        bs, _, H, W = h.shape\n        pos = torch.cat([\n            self.col_embed[:W].unsqueeze(0).repeat(H, 1, 1),\n            self.row_embed[:H].unsqueeze(1).repeat(1, W, 1),\n        ], dim=-1).flatten(0, 1).unsqueeze(1)\n\n        # Generating subsequent mask for target\n        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n            self.trg_mask = self.generate_square_subsequent_mask(trg.shape[1]).to(trg.device)\n\n        # Padding mask\n        trg_pad_mask = self.make_len_mask(trg)\n\n        # Getting positional encoding for target\n        trg = self.decoder(trg)\n        trg = self.query_pos(trg)\n        \n        output = self.transformer(pos + 0.1 * h.flatten(2).permute(2, 0, 1), trg.permute(1, 0, 2), tgt_mask=self.trg_mask, \n                                  tgt_key_padding_mask=trg_pad_mask.permute(1, 0))\n\n        return self.vocab(output.transpose(0, 1))\n\n\ndef make_model(vocab_len, hidden_dim=256, nheads=4,\n                 num_encoder_layers=4, num_decoder_layers=4):\n    \n    return OCR(vocab_len, hidden_dim, nheads,\n                 num_encoder_layers, num_decoder_layers)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:49.780624Z","iopub.execute_input":"2025-03-24T17:21:49.781022Z","iopub.status.idle":"2025-03-24T17:21:49.800473Z","shell.execute_reply.started":"2025-03-24T17:21:49.780990Z","shell.execute_reply":"2025-03-24T17:21:49.799520Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nclass PromitoLipiDataset(Dataset):\n    def __init__(self, img_dir, annotation_file, tokenizer, transform=None):\n        self.img_dir = img_dir\n        self.annotations = load_annotations(annotation_file)  # Load annotations\n        self.image_files = list(self.annotations.keys())  # Get image filenames\n        self.tokenizer = tokenizer  # Use the Tokenizer\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        image_name = self.image_files[idx]\n        label_text = self.annotations[image_name]  # Get Bangla text label\n\n        # Load Image\n        img_path = os.path.join(self.img_dir, image_name)\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            img = self.transform(img)\n\n        # Convert text to token indices\n        y_train = self.tokenizer.encode(label_text)\n\n        # Pad sequence to max length\n        y_train = np.pad(y_train, (0, self.tokenizer.maxlen - len(y_train)), mode=\"constant\")\n\n        return img, torch.tensor(y_train, dtype=torch.long)\n\n\n\n# Image Transformations for ResNet-101\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),  # Resize images\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:51.227893Z","iopub.execute_input":"2025-03-24T17:21:51.228206Z","iopub.status.idle":"2025-03-24T17:21:51.235061Z","shell.execute_reply.started":"2025-03-24T17:21:51.228180Z","shell.execute_reply":"2025-03-24T17:21:51.234138Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class Tokenizer:\n    \"\"\"Handles text-token conversion for Bangla OCR.\"\"\"\n\n    def __init__(self, class_mapping, max_text_length=128):\n        self.PAD_TK, self.UNK_TK, self.SOS_TK, self.EOS_TK = \"¶\", \"¤\", \"SOS\", \"EOS\"\n        \n        # Convert class_mapping to a list of characters\n        self.chars = [self.PAD_TK, self.UNK_TK, self.SOS_TK, self.EOS_TK] + list(class_mapping.values())\n        self.vocab_size = len(self.chars)\n        self.maxlen = max_text_length\n\n        # Create character-to-index and index-to-character mappings\n        self.char_to_idx = {c: i for i, c in enumerate(self.chars)}\n        self.idx_to_char = {i: c for c, i in self.char_to_idx.items()}\n\n    def encode(self, text):\n        \"\"\"Encodes Bangla text into token indices.\"\"\"\n        text = ['SOS'] + list(text) + ['EOS']\n        encoded = [self.char_to_idx.get(c, self.char_to_idx[\"¤\"]) for c in text]\n        return np.array(encoded)\n\n    def decode(self, tokens):\n        \"\"\"Decodes token indices back to Bangla text.\"\"\"\n        text = \"\".join([self.idx_to_char.get(i, \"\") for i in tokens])\n        return text.replace(\"SOS\", \"\").replace(\"EOS\", \"\").replace(\"¶\", \"\").replace(\"¤\", \"\")\n\n\n\n# # Bangla Charset (Letters, Digits, and Common Symbols)\n# bangla_charset = \"অইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহড়ঢ়য়ৎঃংঁ\" \\\n#                  \"০১২৩৪৫৬৭৮৯\" \\\n#                  \"ািীেু্র্যক্ষন্তত্রঙ্গস্থস্বক্তস্তন্দচ্ছদ্ধন্ত্রত্তষ্টন্নল্পম্পূৃৈৌ।\"\n\nclass_mapping = {\n    0: 'blank', 1: 'অ', 2: 'ই', 3: 'ঈ', 4: 'উ', 5: 'ঊ', 6: 'ঋ', 7: 'এ', 8: 'ঐ', 9: 'ও', 10: 'ঔ',\n    11: 'ক', 12: 'খ', 13: 'গ', 14: 'ঘ', 15: 'ঙ', 16: 'চ', 17: 'ছ', 18: 'জ', 19: 'ঝ', 20: 'ঞ', 21: 'ট',\n    22: 'ঠ', 23: 'ড', 24: 'ঢ', 25: 'ণ', 26: 'ত', 27: 'থ', 28: 'দ', 29: 'ধ', 30: 'ন', 31: 'প', 32: 'ফ',\n    33: 'ব', 34: 'ভ', 35: 'ম', 36: 'য', 37: 'র', 38: 'ল', 39: 'শ', 40: 'ষ', 41: 'স', 42: 'হ', 43: 'ড়',\n    44: 'ঢ়', 45: 'য়', 46: 'ৎ', 47: 'ঃ', 48: 'ং', 49: 'ঁ', 50: '০', 51: '১', 52: '২', 53: '৩', 54: '৪',\n    55: '৫', 56: '৬', 57: '৭', 58: '৮', 59: '৯', 60: 'া', 61: 'ি', 62: 'ী', 63: 'ে', 64: 'ু', 65: 'faka',\n    66: '্র', 67: '্য', 68: 'ক্ষ', 69: 'ন্ত', 70: 'ত্র', 71: 'ঙ্গ', 72: 'স্থ', 73: 'স্ব', 74: 'ক্ত',\n    75: 'স্ত', 76: 'ন্দ', 77: 'চ্ছ', 78: 'দ্ধ', 79: 'ন্ত্র', 80: 'ফাকা', 81: 'ত্ত', 82: 'ষ্ট', 83: 'ন্ন',\n    84: 'ল্প', 85: 'ম্প', 86: 'faka', 87: 'ূ', 88: 'ৃ', 89: 'ৈ', 90: 'faka', 91: 'ৌ', 92: '।'\n}\n\n# Initialize Tokenizer with Bangla characters\ntokenizer = Tokenizer(class_mapping)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:51.443347Z","iopub.execute_input":"2025-03-24T17:21:51.443657Z","iopub.status.idle":"2025-03-24T17:21:51.457394Z","shell.execute_reply.started":"2025-03-24T17:21:51.443634Z","shell.execute_reply":"2025-03-24T17:21:51.456376Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"import os\n\ndef load_annotations(annotation_file):\n    \"\"\"Loads annotations from a text file and converts class indices to Bangla text.\"\"\"\n    annotations = {}\n\n    # Check if the file exists\n    if not os.path.exists(annotation_file):\n        print(f\"❌ Error: Annotation file not found at {annotation_file}\")\n        return annotations  # Return empty dictionary\n\n    with open(annotation_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            parts = line.strip().split(\":\")\n            if len(parts) != 2:\n                print(f\"⚠️ Warning: Skipping malformed line: {line.strip()}\")\n                continue  # Skip corrupt lines\n\n            image_name = parts[0].strip()  # Extract image filename\n\n            try:\n                # Convert index numbers to actual Bangla text\n                label_indices = list(map(int, parts[1].strip().split()))\n                label_text = \"\".join(class_mapping.get(idx, \"\") for idx in label_indices)\n                annotations[image_name] = label_text  # Store result\n            except ValueError:\n                print(f\"⚠️ Warning: Skipping line with invalid numbers: {line.strip()}\")\n\n    return annotations\n\n# Load annotations from file\nannotation_file_path = \"/kaggle/input/promitilipi/imageannotationsid_train.txt\"\nannotations = load_annotations(annotation_file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:51.657322Z","iopub.execute_input":"2025-03-24T17:21:51.657618Z","iopub.status.idle":"2025-03-24T17:21:51.685480Z","shell.execute_reply.started":"2025-03-24T17:21:51.657593Z","shell.execute_reply":"2025-03-24T17:21:51.684833Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"batch_size = 16\n\n# Example Usage:\ndataset = PromitoLipiDataset(\n    img_dir=\"/kaggle/input/promitilipi/preprocessed_images/preprocessed_images\",\n    annotation_file=annotation_file_path,\n    tokenizer=tokenizer,  # Pass the tokenizer\n    transform=transforms.Compose([transforms.ToTensor()])\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:51.818693Z","iopub.execute_input":"2025-03-24T17:21:51.818948Z","iopub.status.idle":"2025-03-24T17:21:51.842616Z","shell.execute_reply.started":"2025-03-24T17:21:51.818928Z","shell.execute_reply":"2025-03-24T17:21:51.841784Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# Define dataset\n# dataset = PromitoLipiDataset(\n#     img_dir=\"/kaggle/input/promitilipi/PromitoLipi2/PromitoLipi2/WordImages(bmp)\",\n#     xml_dir=\"/kaggle/input/promitilipi/PromitoLipi2/PromitoLipi2/WordAnnotations(xml)\",\n#     tokenizer=tokenizer,\n#     transform=transform\n# )\n\n# Define split sizes (80% Train, 10% Validation, 10% Test)\ntrain_size = int(0.8 * len(dataset))\nval_size = int(0.1 * len(dataset))\ntest_size = len(dataset) - train_size - val_size  # Ensures total remains same\n\n# Randomly split dataset\ntrain_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n\n# Create DataLoaders\nbatch_size = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Print dataset sizes\nprint(f\"Training Set: {len(train_dataset)} samples\")\nprint(f\"Validation Set: {len(val_dataset)} samples\")\nprint(f\"Test Set: {len(test_dataset)} samples\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:52.015700Z","iopub.execute_input":"2025-03-24T17:21:52.015965Z","iopub.status.idle":"2025-03-24T17:21:52.023467Z","shell.execute_reply.started":"2025-03-24T17:21:52.015944Z","shell.execute_reply":"2025-03-24T17:21:52.022637Z"}},"outputs":[{"name":"stdout","text":"Training Set: 7864 samples\nValidation Set: 983 samples\nTest Set: 983 samples\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = make_model(vocab_len=tokenizer.vocab_size).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:53.570031Z","iopub.execute_input":"2025-03-24T17:21:53.570369Z","iopub.status.idle":"2025-03-24T17:21:54.676854Z","shell.execute_reply.started":"2025-03-24T17:21:53.570342Z","shell.execute_reply":"2025-03-24T17:21:54.676164Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b7\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"class LabelSmoothing(nn.Module):\n    \"Implement label smoothing.\"\n    def __init__(self, size, padding_idx=0, smoothing=0.0):\n        super(LabelSmoothing, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False)\n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n        \n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()\n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        return self.criterion(x, Variable(true_dist, requires_grad=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:54.677866Z","iopub.execute_input":"2025-03-24T17:21:54.678135Z","iopub.status.idle":"2025-03-24T17:21:54.683867Z","shell.execute_reply.started":"2025-03-24T17:21:54.678104Z","shell.execute_reply":"2025-03-24T17:21:54.683150Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"criterion = LabelSmoothing(size=tokenizer.vocab_size, padding_idx=0, smoothing=0.1).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.00001, weight_decay=0.0004)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:54.685009Z","iopub.execute_input":"2025-03-24T17:21:54.685239Z","iopub.status.idle":"2025-03-24T17:21:54.700808Z","shell.execute_reply.started":"2025-03-24T17:21:54.685220Z","shell.execute_reply":"2025-03-24T17:21:54.700148Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# !pip install jiwer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:59.628866Z","iopub.execute_input":"2025-03-24T17:21:59.629195Z","iopub.status.idle":"2025-03-24T17:21:59.632641Z","shell.execute_reply.started":"2025-03-24T17:21:59.629166Z","shell.execute_reply":"2025-03-24T17:21:59.631871Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:21:59.845394Z","iopub.execute_input":"2025-03-24T17:21:59.845709Z","iopub.status.idle":"2025-03-24T17:21:59.850504Z","shell.execute_reply.started":"2025-03-24T17:21:59.845678Z","shell.execute_reply":"2025-03-24T17:21:59.849841Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"97"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"def train(model, criterion, optimiser, scheduler,dataloader):\n \n    model.train()\n    total_loss = 0\n    for batch, (imgs, labels_y,) in enumerate(dataloader):\n          imgs = imgs.to(device)\n          labels_y = labels_y.to(device)\n    \n          optimiser.zero_grad()\n          output = model(imgs.float(),labels_y.long()[:,:-1])\n \n          norm = (labels_y != 0).sum()\n          loss = criterion(output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size), labels_y[:,1:].contiguous().view(-1).long()) / norm\n \n          loss.backward()\n          torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n          optimizer.step()\n          total_loss += loss.item() * norm\n \n    return total_loss / len(dataloader)\n \nfrom jiwer import cer\n\ndef evaluate(model, criterion, dataloader, tokenizer):\n    model.eval()\n    epoch_loss = 0\n    total_cer = 0  # Track total CER\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y,) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n\n            # Model prediction\n            output = model(imgs.float(), labels_y.long()[:, :-1])\n\n            # Compute loss\n            norm = (labels_y != 0).sum()\n            loss = criterion(\n                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n                labels_y[:, 1:].contiguous().view(-1).long()\n            ) / norm\n\n            epoch_loss += loss.item() * norm\n\n            # Convert model output to text (Decoding)\n            predicted_tokens = output.argmax(-1).cpu().numpy()  # Get best predictions\n            predicted_texts = [tokenizer.decode(pred) for pred in predicted_tokens]  # Convert to text\n            actual_texts = [tokenizer.decode(label.cpu().numpy()) for label in labels_y[:, 1:]]  # Ground truth text\n\n            # Compute CER for this batch\n            batch_cer = sum(cer(a, b) for a, b in zip(actual_texts, predicted_texts)) / len(actual_texts)\n            total_cer += batch_cer\n            total_samples += 1\n\n    # Average CER across all validation samples\n    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n    return epoch_loss / len(dataloader), avg_cer\n\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:22:01.334441Z","iopub.execute_input":"2025-03-24T17:22:01.334769Z","iopub.status.idle":"2025-03-24T17:22:01.343899Z","shell.execute_reply.started":"2025-03-24T17:22:01.334740Z","shell.execute_reply":"2025-03-24T17:22:01.342971Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"import os\n\n# Paths\nannotation_file_path = \"/kaggle/input/promitilipi/imageannotationsid_train.txt\"\nimage_dir = \"/kaggle/input/promitilipi/preprocessed_images/preprocessed_images\"\n\n# Load annotation file\nif not os.path.exists(annotation_file_path):\n    print(f\"❌ Error: Annotation file not found at {annotation_file_path}\")\n    exit()\n\n# Read image filenames from annotation file\nwith open(annotation_file_path, \"r\", encoding=\"utf-8\") as file:\n    annotation_lines = file.readlines()\n\n# Extract image filenames (assuming they are before ':' in each line)\nannotation_files = set(line.split(\":\")[0].strip() for line in annotation_lines)\n\n# Get actual image filenames in the folder\nimage_files = set(os.listdir(image_dir))\n\n# Find missing and extra files\nmissing_files = annotation_files - image_files  # Mentioned in annotations but not found\nextra_files = image_files - annotation_files  # Found in the folder but not in annotations\n\n# Print results\nprint(f\"🔍 Total Images in Annotations: {len(annotation_files)}\")\nprint(f\"📂 Total Images in Folder: {len(image_files)}\")\nprint(f\"❌ Missing Images: {len(missing_files)}\")\nprint(f\"⚠️ Extra Images (not listed in annotations): {len(extra_files)}\")\n\n# Show some missing files if any\nif missing_files:\n    print(\"\\n🚨 Missing Image Files (First 10 shown):\")\n    print(\"\\n\".join(list(missing_files)[:10]))\n\n# Show some extra files if any\nif extra_files:\n    print(\"\\n⚠️ Extra Image Files (First 10 shown):\")\n    print(\"\\n\".join(list(extra_files)[:10]))\n\nprint(\"\\n✅ Cross-check complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:22:01.598265Z","iopub.execute_input":"2025-03-24T17:22:01.598581Z","iopub.status.idle":"2025-03-24T17:22:01.622374Z","shell.execute_reply.started":"2025-03-24T17:22:01.598554Z","shell.execute_reply":"2025-03-24T17:22:01.621649Z"}},"outputs":[{"name":"stdout","text":"🔍 Total Images in Annotations: 9830\n📂 Total Images in Folder: 9830\n❌ Missing Images: 0\n⚠️ Extra Images (not listed in annotations): 0\n\n✅ Cross-check complete!\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"import time\nimport numpy as np\nimport torch\n\n# Function to calculate epoch time\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n# Define model save paths\nbest_loss_model_path = \"/kaggle/working/best_model_loss.pth\"  # ✅ Best model based on Loss\nbest_cer_model_path = \"/kaggle/working/best_model_cer.pth\"    # ✅ Best model based on CER\n\n# Initialize best metrics\nbest_valid_loss = np.inf\nbest_valid_cer = np.inf\nc = 0\n\n# Training loop\nfor epoch in range(50):\n    print(f'Epoch: {epoch+1:02}, Learning Rate: {scheduler.get_last_lr()}')\n\n    start_time = time.time()\n\n    train_loss = train(model, criterion, optimizer, scheduler, train_loader)\n    valid_loss, valid_cer = evaluate(model, criterion, val_loader, tokenizer)  # ✅ Returns CER\n\n    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n\n    c += 1\n\n    # ✅ Save Best Model Based on Loss\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), best_loss_model_path)\n        print(f\"✅ Model saved (Best Loss: {valid_loss:.3f})\")\n        c = 0  # Reset counter\n\n    # ✅ Save Best Model Based on CER\n    if valid_cer < best_valid_cer:\n        best_valid_cer = valid_cer\n        torch.save(model.state_dict(), best_cer_model_path)\n        print(f\"✅ Model saved (Best CER: {valid_cer:.3f})\")\n\n    # ✅ Adjust Learning Rate if No Improvement\n    if c > 4:\n        scheduler.step()\n        c = 0  # Reset counter\n\n    print(f'Time: {epoch_mins}m {epoch_secs}s')\n    print(f'Train Loss: {train_loss:.3f}')\n    print(f'Val   Loss: {valid_loss:.3f}')\n    print(f'CER   Score: {valid_cer:.3f}')  # ✅ Display CER Score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T17:44:28.029174Z","iopub.execute_input":"2025-03-24T17:44:28.029464Z","iopub.status.idle":"2025-03-24T19:34:32.820504Z","shell.execute_reply.started":"2025-03-24T17:44:28.029442Z","shell.execute_reply":"2025-03-24T19:34:32.819568Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 106.111)\n✅ Model saved (Best CER: 0.807)\nTime: 2m 11s\nTrain Loss: 109.968\nVal   Loss: 106.111\nCER   Score: 0.807\nEpoch: 02, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 103.348)\n✅ Model saved (Best CER: 0.769)\nTime: 2m 10s\nTrain Loss: 106.788\nVal   Loss: 103.348\nCER   Score: 0.769\nEpoch: 03, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 101.057)\n✅ Model saved (Best CER: 0.556)\nTime: 2m 11s\nTrain Loss: 104.011\nVal   Loss: 101.057\nCER   Score: 0.556\nEpoch: 04, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 98.289)\nTime: 2m 11s\nTrain Loss: 101.524\nVal   Loss: 98.289\nCER   Score: 0.557\nEpoch: 05, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 96.327)\nTime: 2m 11s\nTrain Loss: 98.957\nVal   Loss: 96.327\nCER   Score: 0.612\nEpoch: 06, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 94.494)\nTime: 2m 10s\nTrain Loss: 97.267\nVal   Loss: 94.494\nCER   Score: 1.279\nEpoch: 07, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 92.720)\nTime: 2m 11s\nTrain Loss: 94.747\nVal   Loss: 92.720\nCER   Score: 0.828\nEpoch: 08, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 91.112)\nTime: 2m 12s\nTrain Loss: 92.449\nVal   Loss: 91.112\nCER   Score: 0.797\nEpoch: 09, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 89.594)\nTime: 2m 11s\nTrain Loss: 90.138\nVal   Loss: 89.594\nCER   Score: 0.572\nEpoch: 10, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 87.370)\nTime: 2m 11s\nTrain Loss: 88.633\nVal   Loss: 87.370\nCER   Score: 0.557\nEpoch: 11, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 85.221)\nTime: 2m 11s\nTrain Loss: 86.336\nVal   Loss: 85.221\nCER   Score: 0.642\nEpoch: 12, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 83.608)\n✅ Model saved (Best CER: 0.548)\nTime: 2m 11s\nTrain Loss: 84.291\nVal   Loss: 83.608\nCER   Score: 0.548\nEpoch: 13, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 82.768)\nTime: 2m 11s\nTrain Loss: 82.271\nVal   Loss: 82.768\nCER   Score: 0.894\nEpoch: 14, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 81.412)\nTime: 2m 11s\nTrain Loss: 80.684\nVal   Loss: 81.412\nCER   Score: 0.872\nEpoch: 15, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 79.694)\nTime: 2m 11s\nTrain Loss: 78.680\nVal   Loss: 79.694\nCER   Score: 0.685\nEpoch: 16, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 78.888)\nTime: 2m 11s\nTrain Loss: 76.997\nVal   Loss: 78.888\nCER   Score: 0.591\nEpoch: 17, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 76.716)\nTime: 2m 11s\nTrain Loss: 74.991\nVal   Loss: 76.716\nCER   Score: 0.648\nEpoch: 18, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 75.993)\nTime: 2m 10s\nTrain Loss: 73.687\nVal   Loss: 75.993\nCER   Score: 0.580\nEpoch: 19, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 74.702)\nTime: 2m 11s\nTrain Loss: 71.865\nVal   Loss: 74.702\nCER   Score: 0.806\nEpoch: 20, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 74.500)\n✅ Model saved (Best CER: 0.507)\nTime: 2m 10s\nTrain Loss: 70.193\nVal   Loss: 74.500\nCER   Score: 0.507\nEpoch: 21, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 72.704)\nTime: 2m 10s\nTrain Loss: 68.487\nVal   Loss: 72.704\nCER   Score: 0.590\nEpoch: 22, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 72.278)\nTime: 2m 11s\nTrain Loss: 67.427\nVal   Loss: 72.278\nCER   Score: 0.703\nEpoch: 23, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 71.626)\nTime: 2m 11s\nTrain Loss: 65.375\nVal   Loss: 71.626\nCER   Score: 0.599\nEpoch: 24, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 70.251)\nTime: 2m 10s\nTrain Loss: 63.926\nVal   Loss: 70.251\nCER   Score: 0.649\nEpoch: 25, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 68.936)\nTime: 2m 11s\nTrain Loss: 62.510\nVal   Loss: 68.936\nCER   Score: 0.731\nEpoch: 26, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 67.648)\nTime: 2m 10s\nTrain Loss: 61.002\nVal   Loss: 67.648\nCER   Score: 0.591\nEpoch: 27, Learning Rate: [1e-05]\nTime: 2m 10s\nTrain Loss: 59.360\nVal   Loss: 67.923\nCER   Score: 0.855\nEpoch: 28, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 67.336)\nTime: 2m 10s\nTrain Loss: 58.612\nVal   Loss: 67.336\nCER   Score: 0.749\nEpoch: 29, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 66.712)\nTime: 2m 10s\nTrain Loss: 56.746\nVal   Loss: 66.712\nCER   Score: 0.728\nEpoch: 30, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 66.047)\nTime: 2m 12s\nTrain Loss: 55.771\nVal   Loss: 66.047\nCER   Score: 0.730\nEpoch: 31, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 64.771)\n✅ Model saved (Best CER: 0.507)\nTime: 2m 15s\nTrain Loss: 54.354\nVal   Loss: 64.771\nCER   Score: 0.507\nEpoch: 32, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 64.605)\nTime: 2m 11s\nTrain Loss: 53.126\nVal   Loss: 64.605\nCER   Score: 0.654\nEpoch: 33, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 63.993)\nTime: 2m 10s\nTrain Loss: 51.735\nVal   Loss: 63.993\nCER   Score: 0.638\nEpoch: 34, Learning Rate: [1e-05]\nTime: 2m 10s\nTrain Loss: 50.801\nVal   Loss: 64.007\nCER   Score: 0.791\nEpoch: 35, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 63.144)\nTime: 2m 10s\nTrain Loss: 49.381\nVal   Loss: 63.144\nCER   Score: 0.671\nEpoch: 36, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 62.655)\nTime: 2m 10s\nTrain Loss: 48.339\nVal   Loss: 62.655\nCER   Score: 0.685\nEpoch: 37, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 62.030)\nTime: 2m 10s\nTrain Loss: 46.999\nVal   Loss: 62.030\nCER   Score: 0.861\nEpoch: 38, Learning Rate: [1e-05]\nTime: 2m 11s\nTrain Loss: 45.973\nVal   Loss: 62.144\nCER   Score: 0.947\nEpoch: 39, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 61.961)\nTime: 2m 10s\nTrain Loss: 45.097\nVal   Loss: 61.961\nCER   Score: 0.749\nEpoch: 40, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 61.570)\nTime: 2m 10s\nTrain Loss: 44.132\nVal   Loss: 61.570\nCER   Score: 0.813\nEpoch: 41, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 61.443)\nTime: 2m 10s\nTrain Loss: 42.882\nVal   Loss: 61.443\nCER   Score: 0.880\nEpoch: 42, Learning Rate: [1e-05]\n✅ Model saved (Best Loss: 60.515)\nTime: 2m 12s\nTrain Loss: 42.288\nVal   Loss: 60.515\nCER   Score: 0.909\nEpoch: 43, Learning Rate: [1e-05]\nTime: 2m 12s\nTrain Loss: 40.922\nVal   Loss: 60.970\nCER   Score: 0.709\nEpoch: 44, Learning Rate: [1e-05]\nTime: 2m 12s\nTrain Loss: 40.177\nVal   Loss: 61.632\nCER   Score: 0.868\nEpoch: 45, Learning Rate: [1e-05]\nTime: 2m 12s\nTrain Loss: 38.969\nVal   Loss: 60.959\nCER   Score: 1.004\nEpoch: 46, Learning Rate: [1e-05]\nTime: 2m 12s\nTrain Loss: 38.578\nVal   Loss: 60.566\nCER   Score: 0.708\nEpoch: 47, Learning Rate: [1e-05]\nTime: 2m 13s\nTrain Loss: 37.785\nVal   Loss: 61.128\nCER   Score: 0.767\nEpoch: 48, Learning Rate: [9.5e-06]\n✅ Model saved (Best Loss: 60.402)\nTime: 2m 12s\nTrain Loss: 36.672\nVal   Loss: 60.402\nCER   Score: 0.782\nEpoch: 49, Learning Rate: [9.5e-06]\nTime: 2m 10s\nTrain Loss: 35.387\nVal   Loss: 60.536\nCER   Score: 0.864\nEpoch: 50, Learning Rate: [9.5e-06]\n✅ Model saved (Best Loss: 59.690)\nTime: 2m 11s\nTrain Loss: 35.331\nVal   Loss: 59.690\nCER   Score: 0.741\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Load the best model based on CER\nbest_cer_model_path = \"/kaggle/working/best_model_cer.pth\"  # ✅ Path to Best CER Model\n\n# Initialize model\nmodel = make_model(vocab_len=tokenizer.vocab_size).to(device)  # ✅ Recreate model architecture\nmodel.load_state_dict(torch.load(best_cer_model_path, map_location=device))  # ✅ Load weights\nmodel.eval()  # Set to evaluation mode\n\nprint(\"✅ Best CER Model Loaded Successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:41:39.003305Z","iopub.execute_input":"2025-03-24T19:41:39.003699Z","iopub.status.idle":"2025-03-24T19:41:40.664287Z","shell.execute_reply.started":"2025-03-24T19:41:39.003668Z","shell.execute_reply":"2025-03-24T19:41:40.663382Z"}},"outputs":[{"name":"stdout","text":"Loaded pretrained weights for efficientnet-b7\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-58-1412919b4cc9>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_cer_model_path, map_location=device))  # ✅ Load weights\n","output_type":"stream"},{"name":"stdout","text":"✅ Best CER Model Loaded Successfully!\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"from jiwer import cer\n\ndef test_model(model, criterion, dataloader, tokenizer):\n    model.eval()\n    total_loss = 0\n    total_cer = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n\n            # Get model predictions\n            output = model(imgs.float(), labels_y.long()[:, :-1])\n\n            # Compute loss\n            norm = (labels_y != 0).sum()\n            loss = criterion(\n                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n                labels_y[:, 1:].contiguous().view(-1).long()\n            ) / norm\n            total_loss += loss.item() * norm\n\n            # Convert predictions to text\n            predicted_tokens = output.argmax(-1).cpu().numpy()  # Get best predictions\n            predicted_texts = [tokenizer.decode(pred) for pred in predicted_tokens]  # Convert to text\n            actual_texts = [tokenizer.decode(label.cpu().numpy()) for label in labels_y[:, 1:]]  # Ground truth\n\n            # Compute CER for this batch\n            batch_cer = sum(cer(a, b) for a, b in zip(actual_texts, predicted_texts)) / len(actual_texts)\n            total_cer += batch_cer\n            total_samples += 1\n\n    # Compute average CER\n    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n    return avg_cer\n\n# # 🔥 Compute CER on the test dataset\ntest_cer = test_model(model, criterion, test_loader, tokenizer)\n\nprint(f\"✅ Test CER Score: {test_cer:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:41:44.655230Z","iopub.execute_input":"2025-03-24T19:41:44.655533Z","iopub.status.idle":"2025-03-24T19:41:59.975683Z","shell.execute_reply.started":"2025-03-24T19:41:44.655510Z","shell.execute_reply":"2025-03-24T19:41:59.974708Z"}},"outputs":[{"name":"stdout","text":"✅ Test CER Score: 0.459\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"from jiwer import cer\n\ndef test_model_with_logging(model, criterion, dataloader, tokenizer):\n    model.eval()\n    total_loss = 0\n    total_cer = 0\n    total_samples = 0\n    image_results = []  # Store results for each image\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n\n            # Get model predictions\n            output = model(imgs.float(), labels_y.long()[:, :-1])\n\n            # Compute loss\n            norm = (labels_y != 0).sum()\n            loss = criterion(\n                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n                labels_y[:, 1:].contiguous().view(-1).long()\n            ) / norm\n            total_loss += loss.item() * norm\n\n            # Convert predictions to text\n            predicted_tokens = output.argmax(-1).cpu().numpy()  # Get best predictions\n            predicted_texts = [tokenizer.decode(pred) for pred in predicted_tokens]  # Convert to text\n            actual_texts = [tokenizer.decode(label.cpu().numpy()) for label in labels_y[:, 1:]]  # Ground truth\n\n            # Compute CER for this batch\n            for actual, predicted in zip(actual_texts, predicted_texts):\n                img_cer = cer(actual, predicted)  # CER for this image\n                total_cer += img_cer\n                total_samples += 1\n                \n                # Store for printing\n                image_results.append({\n                    \"Actual Text\": actual,\n                    \"Predicted Text\": predicted,\n                    \"CER\": img_cer\n                })\n\n    # Compute average CER\n    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n\n    # Print results for each image\n    print(\"\\n🔍 **Detailed Image-wise CER Analysis**\")\n    for idx, result in enumerate(image_results[:10]):  # Show first 10 samples\n        print(f\"📌 Image {idx + 1}:\")\n        print(f\"✅ Actual Text    : {result['Actual Text']}\")\n        print(f\"🔠 Predicted Text : {result['Predicted Text']}\")\n        print(f\"📉 CER for Image  : {result['CER']:.3f}\\n\")\n    \n    print(f\"✅ **Final Test CER Score: {avg_cer:.3f}**\")\n    return avg_cer\n\n# 🔥 Run CER calculation with logging\ntest_cer = test_model_with_logging(model, criterion, test_loader, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:41:59.976945Z","iopub.execute_input":"2025-03-24T19:41:59.977353Z","iopub.status.idle":"2025-03-24T19:42:05.729404Z","shell.execute_reply.started":"2025-03-24T19:41:59.977321Z","shell.execute_reply":"2025-03-24T19:42:05.728514Z"}},"outputs":[{"name":"stdout","text":"\n🔍 **Detailed Image-wise CER Analysis**\n📌 Image 1:\n✅ Actual Text    : ঘরাাত\n🔠 Predicted Text : হরাতত\n📉 CER for Image  : 0.400\n\n📌 Image 2:\n✅ Actual Text    : েদখাও\n🔠 Predicted Text : েখোরয়\n📉 CER for Image  : 0.800\n\n📌 Image 3:\n✅ Actual Text    : ৩\n🔠 Predicted Text : ৩\n📉 CER for Image  : 0.000\n\n📌 Image 4:\n✅ Actual Text    : ৈমএী\n🔠 Predicted Text : ৈমী\n📉 CER for Image  : 0.250\n\n📌 Image 5:\n✅ Actual Text    : তও\n🔠 Predicted Text : ০\n📉 CER for Image  : 1.000\n\n📌 Image 6:\n✅ Actual Text    : ৈমীহনত\n🔠 Predicted Text : ৈমীহনত\n📉 CER for Image  : 0.000\n\n📌 Image 7:\n✅ Actual Text    : অামােদর\n🔠 Predicted Text : েেেেেক\n📉 CER for Image  : 0.857\n\n📌 Image 8:\n✅ Actual Text    : েকেনা\n🔠 Predicted Text : েথথথ\n📉 CER for Image  : 0.800\n\n📌 Image 9:\n✅ Actual Text    : ০\n🔠 Predicted Text : ০\n📉 CER for Image  : 0.000\n\n📌 Image 10:\n✅ Actual Text    : দদিরর\n🔠 Predicted Text : িিরর\n📉 CER for Image  : 0.400\n\n✅ **Final Test CER Score: 0.461**\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"# Skip texts with length > 2","metadata":{}},{"cell_type":"code","source":"from jiwer import cer\n\ndef test_model_with_logging(model, criterion, dataloader, tokenizer):\n    model.eval()\n    total_loss = 0\n    total_cer = 0\n    total_samples = 0\n    image_results = []  # Store results for each image\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n\n            # Get model predictions\n            output = model(imgs.float(), labels_y.long()[:, :-1])\n\n            # Compute loss\n            norm = (labels_y != 0).sum()\n            loss = criterion(\n                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n                labels_y[:, 1:].contiguous().view(-1).long()\n            ) / norm\n            total_loss += loss.item() * norm\n\n            # Convert predictions to text\n            predicted_tokens = output.argmax(-1).cpu().numpy()  # Get best predictions\n            predicted_texts = [tokenizer.decode(pred) for pred in predicted_tokens]  # Convert to text\n            actual_texts = [tokenizer.decode(label.cpu().numpy()) for label in labels_y[:, 1:]]  # Ground truth\n\n            # Compute CER for this batch (Skipping short actual texts)\n            for actual, predicted in zip(actual_texts, predicted_texts):\n                if len(actual.strip()) > 2:  # Skip texts with length ≤ 2\n                    continue\n\n                img_cer = cer(actual, predicted)  # CER for this image\n                total_cer += img_cer\n                total_samples += 1\n                \n                # Store for printing\n                image_results.append({\n                    \"Actual Text\": actual,\n                    \"Predicted Text\": predicted,\n                    \"CER\": img_cer\n                })\n    print(\"len :\", len(image_results))\n    # Compute average CER\n    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n\n    # Print results for each image\n    print(\"\\n🔍 **Detailed Image-wise CER Analysis**\")\n    for idx, result in enumerate(image_results[:3]):  # Show first 10 samples\n        print(f\"📌 Image {idx + 1}:\")\n        print(f\"✅ Actual Text    : {result['Actual Text']}\")\n        print(f\"🔠 Predicted Text : {result['Predicted Text']}\")\n        print(f\"📉 CER for Image  : {result['CER']:.3f}\\n\")\n    \n    print(f\"✅ **Final Test CER Score: {avg_cer:.3f}**\")\n    return avg_cer\n\n# 🔥 Run CER calculation with logging\ntest_cer = test_model_with_logging(model, criterion, test_loader, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:42:05.730962Z","iopub.execute_input":"2025-03-24T19:42:05.731253Z","iopub.status.idle":"2025-03-24T19:42:10.384307Z","shell.execute_reply.started":"2025-03-24T19:42:05.731230Z","shell.execute_reply":"2025-03-24T19:42:10.383453Z"}},"outputs":[{"name":"stdout","text":"len : 532\n\n🔍 **Detailed Image-wise CER Analysis**\n📌 Image 1:\n✅ Actual Text    : ৩\n🔠 Predicted Text : ৩\n📉 CER for Image  : 0.000\n\n📌 Image 2:\n✅ Actual Text    : তও\n🔠 Predicted Text : ০\n📉 CER for Image  : 1.000\n\n📌 Image 3:\n✅ Actual Text    : ০\n🔠 Predicted Text : ০\n📉 CER for Image  : 0.000\n\n✅ **Final Test CER Score: 0.105**\n","output_type":"stream"}],"execution_count":61},{"cell_type":"markdown","source":"# Skip texts with length ≤ 2","metadata":{}},{"cell_type":"code","source":"from jiwer import cer\n\n\ndef test_model_with_logging(model, criterion, dataloader, tokenizer):\n    model.eval()\n    total_loss = 0\n    total_cer = 0\n    total_samples = 0\n    image_results = []  # Store results for each image\n\n    with torch.no_grad():\n        for batch, (imgs, labels_y) in enumerate(dataloader):\n            imgs = imgs.to(device)\n            labels_y = labels_y.to(device)\n\n            # Get model predictions\n            output = model(imgs.float(), labels_y.long()[:, :-1])\n\n            # Compute loss\n            norm = (labels_y != 0).sum()\n            loss = criterion(\n                output.log_softmax(-1).contiguous().view(-1, tokenizer.vocab_size),\n                labels_y[:, 1:].contiguous().view(-1).long()\n            ) / norm\n            total_loss += loss.item() * norm\n\n            # Convert predictions to text\n            predicted_tokens = output.argmax(-1).cpu().numpy()  # Get best predictions\n            predicted_texts = [tokenizer.decode(pred) for pred in predicted_tokens]  # Convert to text\n            actual_texts = [tokenizer.decode(label.cpu().numpy()) for label in labels_y[:, 1:]]  # Ground truth\n\n            # Compute CER for this batch (Skipping short actual texts)\n            for actual, predicted in zip(actual_texts, predicted_texts):\n                if len(actual.strip()) <=2:  # Skip texts with length ≤ 2\n                    continue\n\n                img_cer = cer(actual, predicted)  # CER for this image\n                total_cer += img_cer\n                total_samples += 1\n                \n                # Store for printing\n                image_results.append({\n                    \"Actual Text\": actual,\n                    \"Predicted Text\": predicted,\n                    \"CER\": img_cer\n                })\n\n    print(\"len :\", len(image_results))\n\n    # Compute average CER\n    avg_cer = total_cer / total_samples if total_samples > 0 else 0\n\n    # Print results for each image\n    print(\"\\n🔍 **Detailed Image-wise CER Analysis**\")\n    for idx, result in enumerate(image_results[:3]):  # Show first 10 samples\n        print(f\"📌 Image {idx + 1}:\")\n        print(f\"✅ Actual Text    : {result['Actual Text']}\")\n        print(f\"🔠 Predicted Text : {result['Predicted Text']}\")\n        print(f\"📉 CER for Image  : {result['CER']:.3f}\\n\")\n    \n    print(f\"✅ **Final Test CER Score: {avg_cer:.3f}**\")\n    return avg_cer\n\n# 🔥 Run CER calculation with logging\ntest_cer = test_model_with_logging(model, criterion, test_loader, tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:42:10.385408Z","iopub.execute_input":"2025-03-24T19:42:10.385743Z","iopub.status.idle":"2025-03-24T19:42:15.141981Z","shell.execute_reply.started":"2025-03-24T19:42:10.385710Z","shell.execute_reply":"2025-03-24T19:42:15.141139Z"}},"outputs":[{"name":"stdout","text":"len : 451\n\n🔍 **Detailed Image-wise CER Analysis**\n📌 Image 1:\n✅ Actual Text    : ঘরাাত\n🔠 Predicted Text : হরাতত\n📉 CER for Image  : 0.400\n\n📌 Image 2:\n✅ Actual Text    : েদখাও\n🔠 Predicted Text : েখোরয়\n📉 CER for Image  : 0.800\n\n📌 Image 3:\n✅ Actual Text    : ৈমএী\n🔠 Predicted Text : ৈমী\n📉 CER for Image  : 0.250\n\n✅ **Final Test CER Score: 0.881**\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}