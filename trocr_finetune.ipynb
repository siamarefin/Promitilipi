{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11016935,"sourceType":"datasetVersion","datasetId":6844200}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:32:14.464327Z","iopub.execute_input":"2025-03-13T15:32:14.464643Z","iopub.status.idle":"2025-03-13T15:32:14.796559Z","shell.execute_reply.started":"2025-03-13T15:32:14.464616Z","shell.execute_reply":"2025-03-13T15:32:14.795625Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport shutil\n\n# ✅ Paths (Modify based on actual Kaggle structure)\nword_images_dir = \"/kaggle/input/promitilipi/PromitoLipi2/PromitoLipi2/WordImages(bmp)\"\nword_annotations_dir = \"/kaggle/input/promitilipi/PromitoLipi2/PromitoLipi2/WordAnnotations(xml)\"\noutput_dir = \"/kaggle/working/IAM_Format/\"\n\n# ✅ Create output folders\niam_images_dir = os.path.join(output_dir, \"images\")\nos.makedirs(iam_images_dir, exist_ok=True)\n\n# ✅ IAM annotations file\niam_annotations_file = os.path.join(output_dir, \"annotations.txt\")\n\n# ✅ Bangla Character Mapping\nclass_mapping = {\n    0: 'blank', 1: 'অ', 2: 'ই', 3: 'ঈ', 4: 'উ', 5: 'ঊ', 6: 'ঋ', 7: 'এ', 8: 'ঐ', 9: 'ও', 10: 'ঔ',\n    11: 'ক', 12: 'খ', 13: 'গ', 14: 'ঘ', 15: 'ঙ', 16: 'চ', 17: 'ছ', 18: 'জ', 19: 'ঝ', 20: 'ঞ', 21: 'ট',\n    22: 'ঠ', 23: 'ড', 24: 'ঢ', 25: 'ণ', 26: 'ত', 27: 'থ', 28: 'দ', 29: 'ধ', 30: 'ন', 31: 'প', 32: 'ফ',\n    33: 'ব', 34: 'ভ', 35: 'ম', 36: 'য', 37: 'র', 38: 'ল', 39: 'শ', 40: 'ষ', 41: 'স', 42: 'হ', 43: 'ড়',\n    44: 'ঢ়', 45: 'য়', 46: 'ৎ', 47: 'ঃ', 48: 'ং', 49: 'ঁ', 50: '০', 51: '১', 52: '২', 53: '৩', 54: '৪',\n    55: '৫', 56: '৬', 57: '৭', 58: '৮', 59: '৯', 60: 'া', 61: 'ি', 62: 'ী', 63: 'ে', 64: 'ু', 65: 'faka',\n    66: '্র', 67: '্য', 68: 'ক্ষ', 69: 'ন্ত', 70: 'ত্র', 71: 'ঙ্গ', 72: 'স্থ', 73: 'স্ব', 74: 'ক্ত',\n    75: 'স্ত', 76: 'ন্দ', 77: 'চ্ছ', 78: 'দ্ধ', 79: 'ন্ত্র', 80: 'ফাকা', 81: 'ত্ত', 82: 'ষ্ট', 83: 'ন্ন',\n    84: 'ল্প', 85: 'ম্প', 86: 'faka', 87: 'ূ', 88: 'ৃ', 89: 'ৈ', 90: 'faka', 91: 'ৌ', 92: '।'\n}\n\ndef extract_text_from_xml(xml_path):\n    \"\"\"Extracts text from XML annotation file and converts class indices to Bangla words.\"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n\n        # Extract all <name> values from <object> elements\n        objects = root.findall(\"object\")\n        labels = [obj.find(\"name\").text for obj in objects if obj.find(\"name\") is not None]\n\n        # Convert index labels (e.g., \"11 31 62\") to Bangla text using class_mapping\n        bangla_text = \"\".join([class_mapping.get(int(label), \"\") for label in labels if label.isdigit()])\n\n        return bangla_text if bangla_text.strip() else \"\"  # Return text if not empty\n    except ET.ParseError:\n        print(f\"❌ Error parsing {xml_path}\")\n        return \"\"\n\n# # ✅ Process dataset and save in IAM format\n# with open(iam_annotations_file, \"w\", encoding=\"utf-8\") as f:\n#     for xml_file in sorted(os.listdir(word_annotations_dir)):\n#         if not xml_file.endswith(\".xml\"):\n#             continue  # Skip non-XML files\n\n#         xml_path = os.path.join(word_annotations_dir, xml_file)\n#         img_name = xml_file.replace(\".xml\", \".bmp\")\n#         img_path = os.path.join(word_images_dir, img_name)\n\n#         # Extract text annotation\n#         text_label = extract_text_from_xml(xml_path)\n#         if not text_label.strip():\n#             continue  # Skip images with no text\n\n#         # ✅ Convert image format to PNG for IAM dataset\n#         new_img_name = img_name.replace(\".bmp\", \".png\")\n#         new_img_path = os.path.join(iam_images_dir, new_img_name)\n#         shutil.copy(img_path, new_img_path)\n\n#         # ✅ Save annotation in IAM format (Image Name + Label)\n#         f.write(f\"{new_img_name} {text_label}\\n\")\n\nprint(\"✅ Conversion to IAM format completed!\")\nprint(f\"Annotations saved in: {iam_annotations_file}\")\nprint(f\"Images saved in: {iam_images_dir}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!pip install -q datasets jiwer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# ✅ Define the correct file path\nannotation_file_path = \"/kaggle/input/promitilipi/IAM_annotations.txt\"\n\n# ✅ Read the annotations file into a DataFrame\ndf = pd.read_csv(annotation_file_path, delimiter=\" \", header=None, names=[\"file_name\", \"text\"], quoting=3)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = df.head(200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.2)\n# we reset the indices to start from zero\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass IAMDataset(Dataset):\n    def __init__(self, root_dir, df, processor, max_target_length=128):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # get file name + text \n        file_name = self.df['file_name'][idx]\n        text = self.df['text'][idx]\n        # prepare image (i.e. resize + normalize)\n        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n        # add labels (input_ids) by encoding the text\n        labels = self.processor.tokenizer(text, \n                                          padding=\"max_length\", \n                                          max_length=self.max_target_length).input_ids\n        # important: make sure that PAD tokens are ignored by the loss function\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n        return encoding","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrOCRProcessor\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\ntrain_dataset = IAMDataset(root_dir='/kaggle/input/promitilipi/IAM_Format/images/',\n                           df=train_df,\n                           processor=processor)\neval_dataset = IAMDataset(root_dir='/kaggle/input/promitilipi/IAM_Format/images/',\n                           df=test_df,\n                           processor=processor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(eval_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoding = train_dataset[0]\nfor k,v in encoding.items():\n  print(k, v.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\nimage","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = encoding['labels']\nlabels[labels == -100] = processor.tokenizer.pad_token_id\nlabel_str = processor.decode(labels, skip_special_tokens=True)\nprint(label_str)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\neval_dataloader = DataLoader(eval_dataset, batch_size=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate\n\ncer_metric = evaluate.load(\"cer\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_cer(pred_ids, label_ids):\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n\n    return cer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AdamW\nfrom tqdm.notebook import tqdm\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(20):  # loop over the dataset multiple times\n   # train\n   model.train()\n   train_loss = 0.0\n   for batch in tqdm(train_dataloader):\n      # get the inputs\n      for k,v in batch.items():\n        batch[k] = v.to(device)\n\n      # forward + backward + optimize\n      outputs = model(**batch)\n      loss = outputs.loss\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n\n      train_loss += loss.item()\n\n   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n    \n   # evaluate\n   model.eval()\n   valid_cer = 0.0\n   with torch.no_grad():\n     for batch in tqdm(eval_dataloader):\n       # run batch generation\n       outputs = model.generate(batch[\"pixel_values\"].to(device))\n       # compute metrics\n       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n       valid_cer += cer \n\n   print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n\nmodel.save_pretrained(\".\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}